<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Iterative Keypoint Reward: Multi-Step Robotic Manipulation through Real-to-Sim-to-Real">
  <meta name="keywords" content="Robotic Manipulation, Foundation Models, Real2Sim, Sim2Real">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Iterative Keypoint Reward: Multi-Step Robotic Manipulation through Real-to-Sim-to-Real</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Iterative Keypoint Reward: Multi-Step Robotic Manipulation through Real-to-Sim-to-Real</h1>
          <div class="is-size-5 publication-authors">
            <!-- TODO: authors -->
          </div>

          <div class="is-size-5 publication-authors">
            <!-- TODO: affiliations -->
          </div>

          <!-- TODO: change corresponding links below -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
</section>


<section class="hero is-light is-small">
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: rewrite abstract -->
          Specifying reward functions for open-world manipulation tasks in reinforcement learning is a 
          longstanding challenge due to the need to interpret open-ended instructions and translate them 
          into optimizable objectives with rich world knowledge. We introduce \algname (\algabrvname), 
          a visually grounded reward representation that is automatically synthesized and refined by 
          Vision-Language Models (VLMs) for open-world manipulation. Given an RGB-D observation and a 
          free-form language instruction, our method samples keypoints in the scene and leverages VLMs' 
          visual code-generation capabilities to produce Python-based reward functions conditioned on 
          these keypoints. These keypoints capture rich geometric structures, encoding full SE(3) 
          requirements, and enable diverse task objectives by tapping into VLMs' embedded world knowledge. 
          VLMs act as proxies for human behavioral priors, shaping rewards for complex, multi-stage tasks 
          and refining them iteratively based on embodied feedback. We implement a real-to-sim-to-real 
          framework by transferring 3D meshes of real-world objects into simulation, training policies 
          using the generated reward functions, and deploying them back into the real world. Our approach 
          demonstrates efficacy across diverse scenarios, including both prehensile and non-prehensile 
          tasks. Quantitative and qualitative evaluations show the system's ability to autonomously 
          perform complex, long-horizon tasks with human-like capabilities such as multi-step chaining, 
          spontaneous error recovery, and on-the-fly strategy updates. This work provides a scalable 
          solution for automatically generating and refining reward functions for open-world manipulation 
          tasks, bridging simulation and reality, and leveraging VLMs for nuanced control and adaptability.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- TODO: Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>

</section>
<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Overview of IKEA</h2>
  <img src="static/images/method.png" class="method-image" />
  <p class="content has-text-justified">We first obtain keypoints from the scene. These, combined with a 
    human command and execution history, are processed by a Vision-Language Model (VLM) to generate code 
    that defines the reward function. We transfer the scene into simulation, and the reward function is 
    used to train a policy, which is subsequently executed in the real world. This process is repeated 
    across multiple steps to enable long-horizon task execution.</b>
  </p>
</div>

<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Skill Chaining</h2>

  <!-- TODO: descriptions -->
  <p class="content has-text-justified"></p>
  
  <div class="columns">
    <div class="column has-text-centered">
      <video id="dist1"
        controls autoplay loop muted
        width="100%">
        <source src="static/videos/skill_chaining.mp4" 
        type="video/mp4">
      </video>
    </div>
  </div>
</div>

<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Robust to Human Intervention</h2>

  <!-- TODO: descriptions -->
  <p class="content has-text-justified"></p>
  
  <div class="columns">
    <div class="column has-text-centered">
      <video id="dist1"
        controls autoplay loop muted
        width="100%">
        <source src="static/videos/human_intervention.mp4" 
        type="video/mp4">
      </video>
    </div>
  </div>
</div>

<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Self Correction</h2>

  <!-- TODO: descriptions -->
  <p class="content has-text-justified"></p>
  
  <div class="columns">
    <div class="column has-text-centered">
      <video id="dist1"
        controls autoplay loop muted
        width="100%">
        <source src="static/videos/minor_adjustment.mp4" 
        type="video/mp4">
      </video>
    </div>
  </div>
</div>

<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Robust to Human Intervention</h2>

  <!-- TODO: descriptions -->
  <p class="content has-text-justified"></p>
  
  <div class="columns">
    <div class="column has-text-centered">
      <video id="dist1"
        controls autoplay loop muted
        width="100%">
        <source src="static/videos/human_intervention.mp4" 
        type="video/mp4">
      </video>
    </div>
  </div>
</div>

<hr class="rounded">
<div class="rows">
  <h2 class="title is-3">Replan: Learn to Regrasp</h2>

  <!-- TODO: descriptions -->
  <p class="content has-text-justified"></p>
  
  <div class="columns">
    <div class="column has-text-centered">
      <video id="dist1"
        controls autoplay loop muted
        width="100%">
        <source src="static/videos/replanning.mp4" 
        type="video/mp4">
      </video>
    </div>
  </div>
</div>



<!-- TODO: BibTeX -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://nerfies.github.io">Nerfies</a>,
            <a href="https://rekep-robot.github.io">ReKep</a>, and <a href="https://voxposer.github.io/">VoxPoser</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
